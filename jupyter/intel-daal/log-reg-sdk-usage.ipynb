{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Intel® Data Analytics Acceleration Library in Amazon SageMaker\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Intel® Data Analytics Acceleration Library (Intel® DAAL) is the library of Intel® architecture optimized building blocks covering all stages of data analytics: data acquisition from a data source, preprocessing, transformation, data mining, modeling, validation, and decision making. One of its classification algorithms is Logistic Regression.\n",
    "\n",
    "Logistic Regression is a method for modeling the relationships between one or more explanatory variables and a categorical variable by expressing the posterior statistical distribution of the categorical variable via linear functions on observed data. If the categorical variable is binary, that is it takes only two values, \"0\" and \"1\", the Logistic Regression is simple, otherwise, it is multinomial.\n",
    "DAAL Logistic Regression algorithm support L1 and L2 regularizations.\n",
    "\n",
    "\n",
    "Intel® DAAL developer guide: https://software.intel.com/en-us/daal-programming-guide\n",
    "\n",
    "Intel® DAAL documentation for Logistic Regression: https://software.intel.com/en-us/daal-programming-guide-logistic-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Hyperparameters description](#1-bullet)\n",
    "* [Usage of the algorithm](#2-bullet)\n",
    "  * [Upload the data for training](#3-bullet)\n",
    "  * [Creating Training Job using Algorithm ARN](#4-bullet)\n",
    "  * [Live Inference Endpoint for Prediction stage](#5-bullet)\n",
    "  * [Batch transform job](#6-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters description: <a class=\"anchor\" id=\"1-bullet\"></a>\n",
    "\n",
    "<table align=\"left\">\n",
    "    <tr>\n",
    "        <th>Required parameters</th>\n",
    "        <th>Type</th>\n",
    "        <th>Default value</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>nClasses</td>\n",
    "        <td>integer</td>\n",
    "        <td>None</td>\n",
    "        <td>Number of classes in training dataset</td>\n",
    "    </tr>\n",
    "</table> \n",
    "\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "    <tr>\n",
    "        <th>Optional parameters</th>\n",
    "        <th>Type</th>\n",
    "        <th>Default value</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>penaltyL1</td>\n",
    "        <td>float</td>\n",
    "        <td>0</td>\n",
    "        <td>Penalty coefficient for L1 regularization</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>penaltyL2</td>\n",
    "        <td>float</td>\n",
    "        <td>0</td>\n",
    "        <td>Penalty coefficient for L2 regularization</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>interceptFlag</td>\n",
    "        <td>bool</td>\n",
    "        <td>False</td>\n",
    "        <td>A flag that indicates a need to compute θ0j</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>solverName</td>\n",
    "        <td>str</td>\n",
    "        <td>'sgd'</td>\n",
    "        <td>Name of solver that will be used for training stage<br>available values:  'lbfgs', 'adagrad', 'saga', 'sgd'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>solverMethod</td>\n",
    "        <td>str</td>\n",
    "        <td>'defaultDense'</td>\n",
    "        <td>Method of the solver. Available values for 'sgd':<br>\n",
    "               'momentum', 'minibatch', 'defaultDense'<br>\n",
    "                available values for others solver: 'defaultDense'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>solverMaxIterations</td>\n",
    "        <td>integer</td>\n",
    "        <td>100</td>\n",
    "        <td>Max number of iterations for training stage</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>solverAccuracyThreshold</td>\n",
    "        <td>float</td>\n",
    "        <td>1.0-e4</td>\n",
    "        <td>Accuracy of the algorithm. The algorithm terminates when <br>\n",
    "                                   this accuracy is achieved.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>solverBatchSize</td>\n",
    "        <td>integer</td>\n",
    "        <td>number of rows<br>in training dataset</td>\n",
    "        <td>Number of batch indices to compute the stochastic gradient.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>solverLearningRate</td>\n",
    "        <td>float</td>\n",
    "        <td>1.0-e3</td>\n",
    "        <td>learning rate for optimization problem<br>applicable for 'sgd','adagrad', 'saga' only</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>solverStepLength</td>\n",
    "        <td>float</td>\n",
    "        <td>1.0-e3</td>\n",
    "        <td>step size for optimization problem<br>applicable for 'lbfgs' only</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>solverCorrectionPairBatchSize</td>\n",
    "        <td>integer</td>\n",
    "        <td>number of rows<br>in training dataset</td>\n",
    "        <td>Number of batch indices to compute Hessian aproximation.<br>applicable for 'lbfgs' only</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>solverL</td>\n",
    "        <td>integer</td>\n",
    "        <td>1</td>\n",
    "        <td>The number of iterations between<br>calculations of the curvature estimates<br>\n",
    "            applicable for 'lbfgs' only\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    For more detailes please visit:\n",
    "    https://software.intel.com/en-us/daal-programming-guide\n",
    "     \n",
    "    All parameters that start from 'solver' have a name without 'solver' prefix in DAAL documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage of the algorithm <a class=\"anchor\" id=\"2-bullet\"></a>\n",
    "At the first we need to import SageMaker Python package, get execution role and create session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker as sage\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sage.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data for training <a class=\"anchor\" id=\"3-bullet\"></a>\n",
    "When training large models with huge amounts of data, you'll typically use big data tools, like Amazon Athena, AWS Glue, or Amazon EMR, to create your data in S3. For the purposes of this example, we're using some the classic Iris dataset\n",
    "\n",
    "We can use use the tools provided by the SageMaker Python SDK to upload the data to a default bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first download iris-data via sklearn and split it into 'training' and 'test' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.33, random_state=777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the training data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training artifacts will be uploaded at: s3://daal-log-reg-test/log_reg_iris_data_test\n",
      "And data_location will be a parameter for fit method (see training stage below).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://daal-log-reg-test/log_reg_iris_data_test/training_data.csv'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "reshaped_Y_train = y_train.reshape(y_train.shape[0],1)\n",
    "\n",
    "#Last column in training dataset is labels. \n",
    "training_data = np.concatenate((X_train,reshaped_Y_train),axis=1)\n",
    "\n",
    "#save the training data\n",
    "train_data_file = 'training_data.csv'\n",
    "# NO comma at the end of each line in training data.\n",
    "np.savetxt(train_data_file,training_data,delimiter=',')\n",
    "\n",
    "# S3 prefix\n",
    "bucket_name = 'daal-log-reg-test'\n",
    "data_key = 'log_reg_iris_data_test'\n",
    "\n",
    "output_location = 's3://{}/{}'.format(bucket_name, 'output')\n",
    "data_location = output_location = 's3://{}/{}'.format(bucket_name, data_key)\n",
    "print (\"Training artifacts will be uploaded at: \" + output_location)\n",
    "print (\"And data_location will be a parameter for fit method (see training stage below).\")\n",
    "\n",
    "sess.upload_data(train_data_file, bucket=bucket_name, key_prefix=data_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of hyperparameters list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={\"nClasses\": 3,\n",
    "                 \"penaltyL1\": 0,\n",
    "                 \"penaltyL2\": 0,\n",
    "                 \"interceptFlag\": False,\n",
    "                 \"solverBatchSize\":100,   #as training data has 100 samples only, but default value is 150\n",
    "                 #\"solverName\": \"lbfgs\", \n",
    "                 #\"solverMaxIterations\": 1000,\n",
    "                 #\"solverAccuracyThreshold\": 0.0001,\n",
    "                 #\"solverL\": 1\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to create SageMaker Estimator instance with following parameters:\n",
    "<table style=\"border: 1px solid black;\">\n",
    "    <tr>\n",
    "        <td><strong>Parameter name</strong></td>\n",
    "        <td><strong>Description</strong></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>image_name</td>\n",
    "        <td>The container image to use for training</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>role</td>\n",
    "        <td>An AWS IAM role. The SageMaker training jobs and APIs that create SageMaker endpoints use this role to access training data and models</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>train_instance_count</td>\n",
    "        <td>Number of Amazon EC2 instances to use for training. Should be 1, because it is not distributed version of algorithm</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>train_instance_type</td>\n",
    "        <td>Type of EC2 instance to use for training. See available types on Amazon Marketplace page of algorithm</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>input_mode</td>\n",
    "        <td>The input mode that the algorithm supports. May be \"File\" or \"Pipe\"</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>output_path</td>\n",
    "        <td>S3 location for saving the trainig result (model artifacts and output files)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>sagemaker_session</td>\n",
    "        <td>Session object which manages interactions with Amazon SageMaker APIs and any other AWS services needed</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>hyperparameters</td>\n",
    "        <td>Dictionary containing the hyperparameters to initialize this estimator with</td>\n",
    "    </tr>\n",
    "</table>\n",
    "SageMaker Estimator documentation: https://sagemaker.readthedocs.io/en/latest/estimators.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Training Job using Algorithm ARN<a class=\"anchor\" id=\"4-bullet\"></a>\n",
    "Please put in the algorithm arn you want to use below. This can either be an AWS Marketplace algorithm you subscribed to (or) one of the algorithms you created in your own account.\n",
    "The algorithm arn listed below belongs to the Intel DAAL Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "daal_log_reg_arn = \"arn:aws:sagemaker:us-east-2:057799348421:algorithm/intel-daal-logistic-regression-ce8a1f38da2f8a234e4205a356021dbf\" # you can find it on algorithm page in your subscriptions\n",
    "#daal_log_reg_arn = \"<algorithm-arn>\"\n",
    "daal_log_reg = sage.algorithm.AlgorithmEstimator(\n",
    "    algorithm_arn=daal_log_reg_arn,\n",
    "    base_job_name=\"daal-log-reg-alg-test\",\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m4.xlarge',\n",
    "    input_mode=\"File\",\n",
    "    output_path=output_location,\n",
    "    sagemaker_session=sess,\n",
    "    hyperparameters=hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training stage<a class=\"anchor\" id=\"4-bullet\"></a>\n",
    "On training stage, Logistic Rergession algorithm consume input data from S3 location and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: daal-log-reg-alg-test-2018-11-30-10-58-20-329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-30 10:58:20 Starting - Starting the training job...\n",
      "2018-11-30 10:58:25 Starting - Launching requested ML instances......\n",
      "2018-11-30 10:59:21 Starting - Preparing the instances for training......\n",
      "2018-11-30 11:00:40 Downloading - Downloading input data\n",
      "2018-11-30 11:00:40 Training - Downloading the training image..\n",
      "\u001b[31m2018-11-30 11:00:59 INFO     Container setup completed, In Docker entrypoint - train... \u001b[0m\n",
      "\u001b[31m2018-11-30 11:00:59 INFO     Default Hyperparameters loaded: \u001b[0m\n",
      "\u001b[31m2018-11-30 11:00:59 INFO     {'dtype': 'float',\n",
      " 'interceptFlag': True,\n",
      " 'nClasses': 0,\n",
      " 'penaltyL1': 0,\n",
      " 'penaltyL2': 0}\u001b[0m\n",
      "\u001b[31m2018-11-30 11:00:59 INFO     Reading training data... \u001b[0m\n",
      "\u001b[31m2018-11-30 11:00:59 INFO     Training data with labels shape: (100, 5)\u001b[0m\n",
      "\u001b[31m2018-11-30 11:00:59 INFO     Updated with user hyperparameters, Final Hyperparameters: \u001b[0m\n",
      "\u001b[31m2018-11-30 11:00:59 INFO     {'dtype': 'float',\n",
      " 'interceptFlag': 'False',\n",
      " 'nClasses': '3',\n",
      " 'penaltyL1': '0.0',\n",
      " 'penaltyL2': '0.0'}\u001b[0m\n",
      "\u001b[31m2018-11-30 11:00:59 INFO     {'solverAccuracyThreshold': '0.0001',\n",
      " 'solverBatchSize': '100',\n",
      " 'solverCorrectionPairBatchSize': 100,\n",
      " 'solverL': 1,\n",
      " 'solverLearningRate': '0.001',\n",
      " 'solverMaxIterations': '1000',\n",
      " 'solverMethod': 'momentum',\n",
      " 'solverName': 'sgd',\n",
      " 'solverStepLength': 0.001}\u001b[0m\n",
      "\u001b[31m2018-11-30 11:00:59 INFO     If optional parameters were not specified default values will be used.\u001b[0m\n",
      "\u001b[31m2018-11-30 11:00:59 INFO     Training Data Shape: (100, 4)\u001b[0m\n",
      "\u001b[31m2018-11-30 11:00:59 INFO     Training Labels Shape: (100, 1)\u001b[0m\n",
      "\u001b[31m2018-11-30 11:00:59 INFO     Starting DAAL Logistic Regression training...\u001b[0m\n",
      "\u001b[31m2018-11-30 11:00:59 INFO     Training time in sec = 0.040670156478881836\u001b[0m\n",
      "\u001b[31m2018-11-30 11:00:59 INFO     number of classes saved at /opt/ml/model/daal-log-reg-train-features-classes.csv\u001b[0m\n",
      "\u001b[31m2018-11-30 11:00:59 INFO     dtype saved at /opt/ml/model/daal-log-reg-dtype.txt\u001b[0m\n",
      "\u001b[31m2018-11-30 11:00:59 INFO     Model saved at <_io.BufferedWriter name='/opt/ml/model/daal-log-reg-train-model.pkl'>\u001b[0m\n",
      "\n",
      "2018-11-30 11:01:05 Uploading - Uploading generated training model\n",
      "2018-11-30 11:01:05 Completed - Training job completed\n",
      "Billable seconds: 43\n"
     ]
    }
   ],
   "source": [
    "daal_log_reg.fit({\"training\": data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live Inference Endpoint for Prediction stage<a class=\"anchor\" id=\"5-bullet\"></a>\n",
    "On prediction stage, Logistic Regression algorithm compute probabulity of classes and retern the class for each input samples.\n",
    "Firstly, you need to deploy SageMaker endpoint that consumes data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model package with name: intel-daal-logistic-regression-ce8a1f38-2018-11-30-10-05-13-703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: intel-daal-logistic-regression-ce8a1f38-2018-11-30-10-05-59-280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint with name daal-log-reg-alg-test-2018-11-30-10-00-36-782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "predictor = daal_log_reg.deploy(1, 'ml.m4.xlarge', serializer=csv_serializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to handle response from predictor instance at first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_np(prediction, numberOfSamples, nClasses):\n",
    "    if nClasses == 2:\n",
    "        return np.fromstring(prediction, dtype=np.float64, sep=' ').reshape(2,numberOfSamples)\n",
    "    if nClasses > 2:\n",
    "        return np.fromstring(prediction, dtype=np.float64, sep=' ').reshape(nClasses+1,numberOfSamples)\n",
    "\n",
    "def output_to_pd(prediction, nClasses):\n",
    "    C=[]\n",
    "    C.append('lables')\n",
    "    if nClasses > 2:\n",
    "        for i in range(nClasses):\n",
    "            C.append('probability of class ' + str(i))\n",
    "    if nClasses == 2:\n",
    "        C.append('probability of class 1')\n",
    "    return pd.DataFrame(np.transpose(prediction), columns=C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After deployment, you should pass data as numpy array to predictor instance and get predicted lables and probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "0.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "0.0\n",
      "2.0\n",
      "2.0\n",
      "0.0\n",
      "0.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "0.0\n",
      "2.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "0.0\n",
      "2.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "2.0\n",
      "2.0\n",
      "0.0\n",
      "2.0\n",
      "0.0\n",
      "2.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "2.0\n",
      "2.0\n",
      "0.0\n",
      "2.0\n",
      "1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#usage from slplited data\n",
    "payload = X_test\n",
    "\n",
    "ground_truth = y_test\n",
    "prediction = predictor.predict(payload).decode('utf-8')\n",
    "print(prediction)\n",
    "#np_res = output_to_np(prediction=prediction,numberOfSamples=payload.shape[0],nClasses=3)\n",
    "#pd_res = output_to_pd(prediction=np_res, nClasses=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 5 rows of obtained prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lables</th>\n",
       "      <th>probability of class 0</th>\n",
       "      <th>probability of class 1</th>\n",
       "      <th>probability of class 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9.311358e-10</td>\n",
       "      <td>0.010659</td>\n",
       "      <td>9.893406e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.948357e-01</td>\n",
       "      <td>0.005164</td>\n",
       "      <td>1.804341e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.409668e-09</td>\n",
       "      <td>0.018753</td>\n",
       "      <td>9.812474e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.761642e-09</td>\n",
       "      <td>0.003549</td>\n",
       "      <td>9.964515e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.466924e-05</td>\n",
       "      <td>0.858058</td>\n",
       "      <td>1.419178e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lables  probability of class 0  probability of class 1  \\\n",
       "0     2.0            9.311358e-10                0.010659   \n",
       "1     0.0            9.948357e-01                0.005164   \n",
       "2     2.0            2.409668e-09                0.018753   \n",
       "3     2.0            3.761642e-09                0.003549   \n",
       "4     1.0            2.466924e-05                0.858058   \n",
       "\n",
       "   probability of class 2  \n",
       "0            9.893406e-01  \n",
       "1            1.804341e-12  \n",
       "2            9.812474e-01  \n",
       "3            9.964515e-01  \n",
       "4            1.419178e-01  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy of trained model on test dataset 'y_test'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAAL Accuracy on Iris Train Set:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import v_measure_score\n",
    "\n",
    "prediction_arr = np_res[0]\n",
    "ground_truth = y_test\n",
    "#print(prediction_arr)\n",
    "#print(ground_truth)\n",
    "\n",
    "print(\"DAAL Accuracy on Iris Train Set: \", str(v_measure_score(prediction_arr, ground_truth)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to delete endpoint if you don't need it anymore. Otherwise it will run as a daemon process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: daal-log-reg-alg-test-2018-11-30-10-00-36-782\n"
     ]
    }
   ],
   "source": [
    "sess.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch transform job<a class=\"anchor\" id=\"6-bullet\"></a>\n",
    "If you don't need real-time prediction, you can use transform job. It uses saved model, compute transformed data one time and saves it in specified or auto-generated output path.\n",
    "\n",
    "More about transform jobs: https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html\n",
    "\n",
    "Transformer API: https://sagemaker.readthedocs.io/en/latest/transformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model package with name: intel-daal-logistic-regression-ce8a1f38-2018-11-30-10-25-22-669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: intel-daal-logistic-regression-ce8a1f38-2018-11-30-10-26-08-303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transformer = daal_log_reg.transformer(1, \"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: daal-log-reg-alg-test-2018-11-30-10-26-55-276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......................................!\n"
     ]
    }
   ],
   "source": [
    "transformer.transform(\"s3://daal-log-reg-test/input/data/test_data.csv\", content_type=\"text/csv\")\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "2.0\n",
      "1.0\n",
      "0.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "0.0\n",
      "2.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "2.0\n",
      "2.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "0.0\n",
      "0.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "0.0\n",
      "2.0\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "2.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "parsed_url = urlparse(transformer.output_path)\n",
    "bucket_name = parsed_url.netloc\n",
    "file_key = '{}/{}.out'.format(parsed_url.path[1:], \"test_data.csv\") # size of data is equal to 100\n",
    "\n",
    "s3_client = sess.boto_session.client('s3')\n",
    "\n",
    "response = s3_client.get_object(Bucket = sess.default_bucket(), Key = file_key)\n",
    "response_bytes = response['Body'].read().decode('utf-8')\n",
    "print(response_bytes)\n",
    "#size_data = 100\n",
    "#np_res = output_to_np(prediction=response_bytes,numberOfSamples=size_data,nClasses=3)\n",
    "#pd_res = output_to_pd(prediction=np_res, nClasses=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lables</th>\n",
       "      <th>probability of class 0</th>\n",
       "      <th>probability of class 1</th>\n",
       "      <th>probability of class 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.848256e-10</td>\n",
       "      <td>0.008035</td>\n",
       "      <td>0.991965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7.074607e-06</td>\n",
       "      <td>0.810579</td>\n",
       "      <td>0.189414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.953847e-09</td>\n",
       "      <td>0.005116</td>\n",
       "      <td>0.994884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>8.515568e-09</td>\n",
       "      <td>0.014403</td>\n",
       "      <td>0.985597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.201045e-02</td>\n",
       "      <td>0.987550</td>\n",
       "      <td>0.000439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lables  probability of class 0  probability of class 1  \\\n",
       "0     2.0            2.848256e-10                0.008035   \n",
       "1     1.0            7.074607e-06                0.810579   \n",
       "2     2.0            2.953847e-09                0.005116   \n",
       "3     2.0            8.515568e-09                0.014403   \n",
       "4     1.0            1.201045e-02                0.987550   \n",
       "\n",
       "   probability of class 2  \n",
       "0                0.991965  \n",
       "1                0.189414  \n",
       "2                0.994884  \n",
       "3                0.985597  \n",
       "4                0.000439  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_res.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
